{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "import decord\n",
    "from einops import rearrange\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-mpnet-base-v2').cuda().eval()\n",
    "\n",
    "def load_and_select_caption(annotation_path, p_caps_dir, clip_uid=None):\n",
    "    # Load annotations\n",
    "    annotations = json.loads(Path(annotation_path).read_text())\n",
    "    valid_video_id = [annotation['video_id'] for annotation in annotations]\n",
    "\n",
    "    # List p_cap files\n",
    "    p_caps = list(Path(p_caps_dir).glob('*.json'))\n",
    "\n",
    "    # Select a random index\n",
    "    idx = random.randint(0, len(valid_video_id) - 1)\n",
    "\n",
    "    # Find the corresponding p_cap\n",
    "    if clip_uid is not None:\n",
    "        selected_p_cap = None\n",
    "        for p_cap in p_caps:\n",
    "            if clip_uid in p_cap.stem:\n",
    "                selected_p_cap = p_cap\n",
    "                break\n",
    "    else:\n",
    "        selected_p_cap = None\n",
    "        for p_cap in p_caps:\n",
    "            if valid_video_id[idx] in p_cap.stem:\n",
    "                selected_p_cap = p_cap\n",
    "                break\n",
    "\n",
    "    if selected_p_cap is None:\n",
    "        raise ValueError(\"No matching p_cap found.\")\n",
    "\n",
    "    # Load the caption data\n",
    "    cap_data = json.load(selected_p_cap.open())['answers']\n",
    "\n",
    "    # Convert time to seconds\n",
    "    time = [int(entry[0] / 30) for entry in cap_data]\n",
    "\n",
    "    # Extract captions\n",
    "    caps = [entry[2] for entry in cap_data]\n",
    "\n",
    "    return selected_p_cap, cap_data, time, caps\n",
    "\n",
    "def plot_cosine_similarity_heatmap(cos_sim, time, p_cap, interval = 10):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Create the heatmap\n",
    "    sns.heatmap(cos_sim, cmap='viridis')\n",
    "    \n",
    "    # Set x and y ticks\n",
    "    plt.xticks(range(0, len(time), interval), [time[i] for i in range(0, len(time), interval)], rotation=90)\n",
    "    plt.yticks(range(0, len(time), interval), [time[i] for i in range(0, len(time), interval)], rotation=0)\n",
    "    \n",
    "    # Optionally hide axes\n",
    "    # plt.gca().axes.xaxis.set_visible(False)\n",
    "    # plt.gca().axes.yaxis.set_visible(False)\n",
    "    \n",
    "    # Set labels and title\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Time (s)')\n",
    "    plt.title(f'<Cosine Similarity Heatmap>\\nclip_uid: {p_cap.stem}')\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    \n",
    "    # Print clip_uid stem\n",
    "    print(p_cap.stem)\n",
    "    \n",
    "# Example usage\n",
    "annotation_path = '/data/joohyun7u/project/NLQ/nlq_lightning/data/unified/annotations.NLQ_train.json'\n",
    "p_caps_dir = '/data/joohyun7u/project/NLQ/nlq_lightning/data/llava-v1.6-34b/global'\n",
    "# p_caps_dir = '/data/joohyun7u/project/NLQ/nlq_lightning/data/LLaVA-NeXT-Video-7B-DPO/global_v2'\n",
    "selected_p_cap, cap_data, time, caps = load_and_select_caption(annotation_path, p_caps_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_caps_dir = '/data/joohyun7u/project/NLQ/nlq_lightning/data/llava-v1.6-34b/global'\n",
    "# p_caps_dir = '/data/joohyun7u/project/NLQ/nlq_lightning/data/LLaVA-NeXT-Video-7B-DPO/global_v2'\n",
    "selected_p_cap, cap_data, time, caps = load_and_select_caption(annotation_path, p_caps_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "data_dir = '/data/joohyun7u/project/NLQ/nlq_lightning/data/unified'\n",
    "feature_type = 'egovlp_internvideo'\n",
    "\n",
    "video_features = h5py.File(os.path.join(data_dir, feature_type + '.hdf5'), 'r')\n",
    "\n",
    "np.random.seed(0)\n",
    "p_nlq_val_json = Path('/data/joohyun7u/project/NLQ/nlq_lightning//data/unified/annotations.NLQ_val.json')\n",
    "nlq_val_data = json.load(p_nlq_val_json.open())\n",
    "\n",
    "def show_frames(clip_uid):\n",
    "\t# p_clip = Path('/data/datasets/ego4d_data/clips_320p-non_official/') / f'{clip_uid}.mp4'\n",
    "\tp_clip = Path('/data/datasets/ego4d_data/v2/clips/') / f'{clip_uid}.mp4'\n",
    "\tvr = decord.VideoReader(str(p_clip))\n",
    "\tnum_frames = 16\n",
    "\tborder = 10\n",
    "\tframe_idxs = np.arange(len(vr)//num_frames//2, len(vr), len(vr)//num_frames)\n",
    "\tborder_colors = plt.get_cmap('jet')(frame_idxs/len(vr))\n",
    "\tmidframes = vr.get_batch(frame_idxs.tolist()).asnumpy()\n",
    "\tout_frames = midframes.copy()\n",
    "\tfor i, frame in enumerate(midframes):\n",
    "\t\tframe[:border] = border_colors[i][:3] * 255\n",
    "\t\tframe[-border:] = border_colors[i][:3] * 255\n",
    "\t\tframe[:, :border] = border_colors[i][:3] * 255\n",
    "\t\tframe[:, -border:] = border_colors[i][:3] * 255\n",
    "\tmidframes = rearrange(midframes, '(th tw) h w c -> (th h) (tw w) c', tw=num_frames)\n",
    "\tdisplay(Image.fromarray(midframes))\n",
    "\treturn frame_idxs, out_frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_list = list(video_features.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# clip_uid = '13635dfb-dfcc-4ada-bcee-1482a568aa89'\n",
    "# clip_uid = '5e59031d-0deb-4557-a3e1-ba0ba2bb5465'\n",
    "clip_uid = 'a99baf07-ce1c-4f73-ab20-ed0dfc079510'\n",
    "clip_uid = '00d9a297-d967-4d28-8e5a-6b891814ec65'\n",
    "\n",
    "# clip_list = list(video_features.keys())\n",
    "# for i in range(10):\n",
    "clip_uid = random.choice(clip_list)\n",
    "clip_uid = clip_list[40]\n",
    "\n",
    "selected_p_cap, cap_data, time, caps = load_and_select_caption(annotation_path, p_caps_dir, clip_uid=clip_uid)\n",
    "\n",
    "frame_idxs, frames = show_frames(clip_uid)\n",
    "\n",
    "data = video_features[clip_uid][:]\n",
    "print(data.shape)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# CLS 토큰 간 코사인 유사도 계산\n",
    "similarity_matrixs = []\n",
    "# similarity_matrix.append\n",
    "similarity_matrix = cosine_similarity(data)  # shape: (900, 900)\n",
    "# similarity_matrix = cosine_similarity(data[:,:256])  # shape: (900, 900)\n",
    "# similarity_matrix = cosine_similarity(data[:,256:256+1024])  # shape: (900, 900)\n",
    "# similarity_matrix = cosine_similarity(data[:,256+1024:])  # shape: (900, 900)\n",
    "\n",
    "# similarity_matrix = data @ data.T\n",
    "\n",
    "# 히트맵 시각화\n",
    "plt.figure(figsize=(10, 8))\n",
    "# plt.imshow(similarity_matrix, cmap='coolwarm')\n",
    "plt.imshow(similarity_matrix, cmap='viridis')\n",
    "plt.title(f'CLS Token Similarity Heatmap\\nclip_uid: {clip_uid}')\n",
    "plt.colorbar(label='Cosine Similarity')\n",
    "plt.xlabel('CLS Token Index')\n",
    "plt.ylabel('CLS Token Index')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "embeddings = model.encode(caps)\n",
    "cos_sim = embeddings @ embeddings.T\n",
    "# cos_sim = cosine_similarity(embeddings)\n",
    "\n",
    "plot_cosine_similarity_heatmap(cos_sim, time, selected_p_cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/joohyun7u/project/NLQ/nlq_lightning/data/llava-v1.6-34b/global'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_caps_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0a798ad9-e163-4d1a-9a26-0ba6a8dce89e'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
